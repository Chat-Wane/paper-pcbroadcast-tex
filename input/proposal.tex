
\section{Causal broadcast\\for large and dynamic systems}
\label{sec:proposal}

In this section, we introduce \CBROADCAST (stands for Preventive Causal
broadcast), a causal broadcast protocol that breaks scalability barriers for
large and dynamic systems.
% To provide causal order, most
% state-of-the-art~\cite{almeida2008interval,fidge1988timestamps,mattern1989virtual,singhal1992efficient},~\cite{birman1987reliable,hadzilacos1993fault,mostefaoui2017probabilistic}
% approaches are reactive. They check if message deliveries should be delayed to
% avoid causality violations.
Our approach is preventive. Messages are immediately delivered on receipt
without risk of causality violations. This difference not only removes most of
control information piggybacked in broadcast messages, but also leads to
constant delivery execution time. Protocols and applications can finally afford
causal broadcast in large and dynamic systems without loss of efficiency.

% This section states  the definitions. It describes our algorithm. It proves that
% it handles both static and dynamic networks. It analyses its complexity.

\subsection{Model}

% Definitions and theorems come from~\cite{hadzilacos1994modular}.
A distributed system comprise processes. Processes can communicate using
messages. They may not have full knowledge of the membership, for maintenance
costs become too expansive in large and dynamic systems. Instead, processes
build overlay networks with local partial view the size of which is generally
much smaller than the actual size of the
system~\cite{bertier-d2ht,jelasity2007gossip,jelasity2009tman}.

\begin{definition}[Overlay network]
  An overlay network $N$ comprises a set of processes $P$. Each Process runs a
  set of instructions sequentially. \\
  An overlay network $N$ also comprises a set of links $E: P \times P$. $p$'s
  neighborhood $Q$ is the set of links departing from $p$. Processes can
  communicate with their neighbors using messages. \\
  Processes are faulty if they crash, otherwise they are correct. % The set of
  % correct processes is $C$. 
  There are no byzantine processes.
\end{definition}

For the rest of this paper, we will speak of distributed systems, overlay
networks, or networks indifferently.

\begin{definition}[Static and dynamic networks]
  A network is static if both its set of processes and its set of edges are
  immutable. Otherwise, the network is dynamic.
\end{definition}

For the rest of the paper, we only consider networks without partitions.

\begin{definition}[Network partition]
  A network has partitions if there exist two correct processes without any path
  between them, i.e., without a link or a sequence of links comprising correct
  processes only.
\end{definition}

%\TODO{Replace ``network'' by  ``distributed system'' ?}

Processes communicate by sending messages to other processes.  A process A can
send messages to another process B $s_{AB}(m)$; receive a message from another
process B $r_{AB}(m)$; broadcast a message to all processes $b_A(m)$; receive a
broadcast message $r_A(m)$; deliver a broadcast message $d_A(m)$.

\begin{definition}[Uniform reliable broadcast]
  When a process broadcasts a message to all processes of the network, correct
  processes eventually receive it.
  Uniform reliable broadcast guarantees 3 properties: \\
  \textbf{Validity:} If a correct process broadcasts a message, then it
  eventually delivers it. \\
  \textbf{Uniform Agreement:} If a process -- correct or not -- delivers a
  message, then all correct processes eventually deliver it. \\
  \textbf{Uniform Integrity:} A process delivers a message at most once, and
  only if it was previously broadcast.
\end{definition}

\begin{algorithm}[h]
  \input{./input/algoreliablebroadcast.tex}
  \caption{\label{algo:reliablebroadcast}R-broadcast at Process $p$.}
\end{algorithm}

Algorithm~\ref{algo:reliablebroadcast} shows the instructions of a uniform
reliable broadcast. It uses a structure that keeps track of received messages in
order to deliver them at most once. 
%It uses a peer-sampling protocol that
%provides neighbors to communicate with, i.e., a set of links. 
%%Assuming a network without partitions meaning that there exists at least one
%path from any process to any correct process, then all correct processes
%eventually receive all messages at least once:
Since processes may not have full membership knowledge, processes must forward
broadcast messages. Since the network does not have partitions, processes either
receive the message directly from the broadcaster or transitively. Thus, all
correct processes eventually deliver all messages exactly once. R-broadcast
ensures validity, uniform agreement, and uniform integrity.

% At this point, processes receive and deliver each message exactly once. However,
% R-broadcast delivers messages in any order.  In addition to reliably conveying
% messages to all correct processes. 

Broadcast protocols can also ensure a specific ordering among message
deliveries.
%that messages are delivered in a specific order.
To define a delivery order among messages, we define time in a logical sense
using Lamport's definition~\cite{lamport1978time}.

\begin{definition}[Happen before~\cite{lamport1978time}]
  Happen before is a transitive, irreflexive, and antisymmetric relation that
  defines a strict partial orders of events.  The sending of a message always
  precedes its receipt.
\end{definition}

To order messages broadcast from one process, we define FIFO order.

\begin{definition}[FIFO order]
  If a process broadcasts a first message then broadcasts a second message,
  processes deliver the first message before the second message.
\end{definition}

To order messages broadcast by different processes, we define local order.

\begin{definition}[Local order]
  If a process broadcasts a message after having delivered another message
  broadcast by another process, processes deliver the latter before the former.
\end{definition}

To order messages broadcast by every processes, we define causal order.

\begin{definition}[Causal order]
  The delivery order of messages follows the happen before relationships of the
  corresponding broadcasts.
\end{definition}

\begin{theorem}[\label{theo:causal}Causal order equivalence~\cite{hadzilacos1994modular}]
  The conjunction of FIFO order and local order is equivalent to causal order.
\end{theorem}

\begin{definition}[Causal broadcast]
  Causal broadcast is a uniform reliable broadcast ensuring causal order.
\end{definition}

\begin{theorem}[\label{theo:flooding}Constraint flooding in deterministic
  overlay networks is causal~\cite{friedman2004causal}]
  In static networks, a broadcast protocol is causal if it uses FIFO links,
  forwards all broadcast messages exactly once, and uses all its outgoing links.
\end{theorem}

From Theorem~\ref{theo:flooding}, reliable broadcast from
Algorithm~\ref{algo:reliablebroadcast} is causal if communication links employed
to communicate with neighbors in $Q$ are FIFO. This holds only for static
networks where $Q$ is immutable. In practice, processes add and remove links to
neighbors from $Q$ at any time. Next section describes \CBROADCAST, a causal
broadcast that handles such dynamicity.


\subsection{Operation}

\begin{algorithm}[h]
  \input{./input/algobufferbroadcast.tex}
  \caption{\label{algo:bufferbroadcast}\CBROADCAST at Process $p$.}
\end{algorithm}

\begin{figure*}
  \begin{center}
    \subfloat[Part A][\label{fig:preventivesolveA}Process~A broadcasts $a$.]
    {\input{./input/figpreventivesolveA.tex}}
    \hspace{20pt}
    \subfloat[Part B][\label{fig:preventivesolveB}Process~A wants
    to add a link to Process~D. 
    It sends a ping message $\pi$ to Process~D using one of its FIFO links.]
    {\input{./input/figpreventivesolveB.tex}}
    \hspace{20pt}
    \subfloat[Part C][\label{fig:preventivesolveC}Process~A broadcasts $a'$.
    It does not send it through the new link but buffers it.]
    {\input{./input/figpreventivesolveC.tex}}
    \hspace{20pt}
    \subfloat[Part D][\label{fig:preventivesolveD}Process~D receives
    $\pi$ and replies to $A$.
    The reply $\rho$ can travel by any communication
    mean.]
    {\input{./input/figpreventivesolveD.tex}}
    \hspace{20pt}
    \subfloat[Part E][\label{fig:preventivesolveE}Process~A receives
    Process~D's reply. 
    The former safely empties its buffer to Process~D. 
    Using the new link cannot cause causal order violation anymore.]
    {\input{./input/figpreventivesolveE.tex}}
    \caption{\label{fig:preventivesolve}\CBROADCAST does not violate causal
      order in dynamic settings.}
  \end{center}
\end{figure*}

Algorithm~\ref{algo:bufferbroadcast} shows the instructions of \CBROADCAST. Its
two operations broadcast and deliver rely on reliable broadcast (see
Algorithm~\ref{algo:reliablebroadcast}). In fact, without additions nor removals
of links, our protocol only executes instructions of reliable broadcast (see
Line~\ref{line:rbroadcast},~\ref{line:rdeliver}).

\begin{theorem}[\CBROADCAST is causal in static networks\label{theo:static}]
  \CBROADCAST is a causal broadcast in static networks.
\end{theorem}

\begin{proof}
  In static networks, \CBROADCAST only executes the instructions of reliable
  broadcast. Reliable broadcast along with FIFO links ensures causal broadcast.
  The proof can be found in~\cite{friedman2004causal}.
\end{proof}

The removal of links and the departure of processes are not an issue, for it
does not reorder messages traveling through the links\footnote{It may create
  partitions infringing the uniform agreement property. Network partitioning
  constitutes an orthogonal problem that we do not address in this
  paper.}. Algorithm~\ref{algo:bufferbroadcast} does not provide specific
instructions for such cases.
% However, it may create partitions. It is an orthogonal problem

\begin{lemma}[\CBROADCAST is causal in dynamic networks subject to
  removals\label{lem:removals}]
  \CBROADCAST is a causal broadcast in dynamic networks where processes can
  leave the network or links can be removed.
\end{lemma}

\begin{proof}
  Removing a process from the network and removing all the incoming and outgoing
  links of this process is equivalent. We assume that removals do not create
  network partitions.  Removing a link does not change the delivery order of
  causally related messages. Identically to the proof of
  Theorem~\ref{theo:static}, our causal broadcast only executes instructions of
  reliable broadcast in such cases. The proof can be found
  in~\cite{friedman2004causal}.
\end{proof}

Unfortunately, adding links may infringe the causal order property of causal
broadcast.  Figure~\ref{fig:preventiveproblem} shows the issue with link
additions.  New links may act as shortcut for messages. First messages that
travel through new links may arrive before preceding messages that took longer
paths.

To solve this issue, we define the safety of a link. \CBROADCAST uses all and
only safe links to disseminate messages.

\begin{definition}[Safe link]
  A link from Process~A to Process~B is safe if and only if Process~B received
  or will receive all messages delivered by Process~A before receiving any
  message that Process~A will deliver:
  $safe(AB) \equiv \forall m,\, m',\, d_A(m) \rightarrow s_{AB}(m') \implies
  r_B(m) \rightarrow r_{AB}(m')$
\end{definition}

\begin{figure*}
  \begin{center}
    \subfloat[part A][\label{fig:bufferproblemA}Slow ping message $\pi$.]
    {\input{input/figbufferproblemA.tex}}
    \hspace{20pt}
    \subfloat[part B][\label{fig:bufferproblemB}Process~D crashes.]
    {\input{input/figbufferproblemB.tex}}
    \hspace{20pt}
    \subfloat[part C][\label{fig:bufferproblemC}Reply $\rho$ fails.]
    {\input{input/figbufferproblemC.tex}}
    \caption{\label{fig:bufferproblem}Buffers may grow unbounded due to network
      conditions.}
  \end{center}
\end{figure*}




Added links start unsafe. In Figure~\ref{fig:preventiveproblem}, Process~A uses
the link to broadcast $a'$ while it is unsafe: Process~B did not receive $a$
yet, and there was no guaranty that Process~B would receive $a$ before receiving
$a'$ from the new link. In this example, the worst happens and Process~B
receives then delivers $a'$ before $a$ which violates causal order.

The challenge is to make unsafe links safe using local knowledge only. A
straightforward mean for Process~A to achieve this consists in sending all its
delivered messages to Process~B using this unsafe link. This guarantees that any
message delivered by A will be received by B before A starts using the new --
now safe -- link for causal broadcast. However, this is costly both in local
space and generated traffic.

Instead of sending all its delivered messages, Process~A initiates a ping phase
to Process~B while recording all its upcoming message deliveries in an array.

\begin{definition}[Ping phase]
  Ping phase starts when Process~A pings Process~B. Ping messages $\pi$ travel
  using safe links. When Process~B receives this ping, it replies to
  Process~A. Replies $\rho$ travel using any communication mean. Ping phase ends
  when Process~A receives the reply of Process~B.
\end{definition}

\begin{lemma}[\label{lemma:ping}Ping phases acknowledge broadcast receipts]
  A ping phase from Process~A to Process~B acknowledges the receipt by B of all
  messages delivered by Process~A before this ping phase:
  $\forall m,\, d_A(m) \rightarrow s_A(\pi_{AB}) \wedge r_A(\rho_{AB}) \implies
  r_B(m)$
\end{lemma}

\begin{proof}
  Suppose a process~A initiates a ping phase to a process B. Suppose series of
  messages delivered by Process~A. Process~A sent these messages exactly once
  using all its outgoing safe links. Processes that will receive these message
  either already forwarded them or will forward them in their receipt
  order. Since Process~A's ping travels using safe links after these messages,
  when Process~B receives the ping, it already received all messages delivered
  by Process~A. Process~A receives Process~B's reply after Process~B received
  the ping. Consequently, when Process~A receives Process~B's reply, Process~B
  received all messages delivered by Process~A before the start of this ping
  phase.
\end{proof}

Upon receipt of Process~B's reply, Process~A has the guaranty that Process~B
received all its delivered messages preceding the ping phase. This is not
sufficient, for ping phases take time. Messages delivered during ping phase by
Process~A may not be received by Process~B yet. To fill this gap, Process~A
sends to Process~B the messages it buffered during ping phase.


\begin{lemma}[Ping phase and buffering makes safe links]
  Process~A makes an unsafe link to Process~B safe by completing a ping phase to
  Process~B then finalizing it by sending all delivered messages buffered during
  ping phase using the new link.
  %% \TODO{$\forall i,\, m_i \rightarrow m_{i+1},\, ack(m_i) \rightarrow \forall
  %% j>i,\, s_{AB}(m_j) \rightarrow s_{AB}(m_{j+1}) \implies safe(AB)$}
\end{lemma}

\begin{proof}
  Suppose series of messages $m_1 \ldots m_i \ldots m_j$ delivered by a
  process~A. Suppose Process~A initiated a ping phase to a process B after
  delivering $m_i$. Suppose Process~A receives Process~B's reply after $m_j$.
  We must show that when Process~A delivers a message after $m_k$, Process~B
  received or will receive $m_1 \ldots m_j$ before. \\
  From Lemma~\ref{lemma:ping}, when Process~A receives Process~B's reply,
  Process~B received $m_1 \ldots m_i$. \\
  Since Process~A buffered all messages delivered since the beginning of the
  ping phase, the buffer contains $m_{i+1} \ldots m_j$ when the ping phase ends.
  Since links are FIFO, sending messages of this buffer using the new link
  guarantees that Process~B will receive them before receiving any $m_k$
  delivered after $m_j$. The link from Process~A to Process~B became safe.
  % Suppose a network of safe links without partitions. Suppose an unsafe link
  % from a process A to a process B. \\
  % We must show that if Process~B receives a message $m'$ from the new link,
  % there exist no messages $m$ delivered by Process~A that it did not receive. \\
  % Process~B starts receiving messages from the new link when Process~A sends its
  % buffer. Either $m'$ comes from this buffer or it is a message delivered by
  % Process~A afterward. \\
  % Process~A starts sending buffered messages once it received the reply to its
  % ping, hence, after Process~B received this ping. Since this ping travels via
  % safe FIFO links, and since the network does not have partitions, Process~B
  % eventually receives this ping after all messages delivered by Process~A when
  % it sent this ping. Consequently, there exist no messages $m$ that Process~B
  % did not receive before receiving the messages from the buffer. \\
  % Since Process~A buffered all delivered messages between the sending of the
  % ping and the receipt of the reply, there exist no messages $m$ that Process~B
  % did not receive before Process~A starts using the new link for
  % broadcast. \\
  % Afterward, Process~A sends all its delivered messages using this link.
\end{proof}

% Compared to reliable broadcast, Algorithm~\ref{algo:bufferbroadcast} adds a
% structure associating each new unsafe link with a buffer of
% messages.

Figure~\ref{fig:preventivesolve} shows on an example how \CBROADCAST (see
Algorithm~\ref{algo:bufferbroadcast}) solves causal order violations. In
Figure~\ref{fig:preventivesolveA}, Process~A broadcasts $a$.  In
Figure~\ref{fig:preventivesolveB}, it wants to add a link to Process~D. It sends
a ping message $\pi$ to process~D (see Line~\ref{line:sendlocked}) and awaits
for the latter's reply.  We leave aside the implementation of this send function
(e.g. broadcast or routing). % Although, using already safe links constitutes a
%cheap way to achieve it.
While awaiting, Process~A keeps its normal functioning and maintain a buffer of
messages associated with the unsafe link (see
Line~\ref{line:bufferbroadcast},~\ref{line:bufferforward}). In
Figure~\ref{fig:preventivesolveC}, Process~A broadcasts another message $a'$. It
sends it normally to Process~B but does not send it to Process~D
directly. Instead, it buffers it. In Figure~\ref{fig:preventivesolveD},
Process~D receives Process~A's ping message $\pi$. Since links are FIFO, it
implicitly means that Process~D also received $a$. Process~D sends a reply
$\rho$ to Process~A (see Line~\ref{line:sendack}). $\rho$ can travel through any
communication mean. In Figure~\ref{fig:preventivesolveE}, Process~A receives
$\rho$. Consequently, Process~A knows that Process~D received and delivered at
least $a$ and all preceding messages. It empties the buffer of messages to
Process~D (see Line~\ref{line:emptybuffer}). Afterwards, the new link is
safe. Process~A uses the new link normally.

\begin{figure*}
  \begin{center}
    \subfloat[Part A][\label{fig:buffersolveA}
    Process~A wanted to add a link to Process~D after having
    broadcast $a$. Afterwards, it broadcast $a'$ and $a''$.]
    {\input{input/figbuffersolveA.tex}}
    \hspace{20pt}
    \subfloat[Part B][\label{fig:buffersolveB}
    Process~A receives, delivers, and forwards $x$. Since the buffer size would 
    exceed the maximal boundary, it resets the buffer with a new counter and
    sends a ping message with this counter. In the meantime, Process~D
    receives $\pi_1$ and sends the corresponding reply $\rho_1$.]
    {\input{input/figbuffersolveB.tex}}
    \hspace{20pt}
    \subfloat[Part C][\label{fig:buffersolveC}
    Process~A forwards $y$ and buffers it. It also
    receives $\rho_1$ but no buffers exist with this counter. Process~A 
    simply discards $\rho_1$.]
    {\input{input/figbuffersolveC.tex}}
    \hspace{20pt}
    \subfloat[Part D][\label{fig:buffersolveD}
    Process~D receives $\pi_2$ and sends the corresponding
    reply $\rho_2$.]
    {\input{input/figbuffersolveD.tex}}
    \hspace{20pt}
    \subfloat[Part E][\label{fig:buffersolveE}
    Process~A receives $\rho_2$. It empties the
    buffer with the corresponding counter. Now,  the direct link
    to Process~D is safe. Process~A uses it normally.]
    {\input{input/figbuffersolveE.tex}}
    \caption{\label{fig:buffersolve}Buffers become bounded. We allow only 2
      elements in each buffer.}
  \end{center}
\end{figure*}


\begin{lemma}[\CBROADCAST is causal in dynamic networks subject to
  additions\label{lem:additions}]
  \CBROADCAST is a causal broadcast in dynamic networks where processes can join
  the network or links can be added.
\end{lemma}

\begin{proof}
  To prove that \CBROADCAST is a causal broadcast, we must show that it ensures
  validity, uniform agreement, uniform integrity, and causal order. \\
  \textbf{Validity, uniform agreement, uniform integrity:} \CBROADCAST extends
  R-broadcast. Since R-broadcast ensures validity, uniform agreement, and
  uniform
  integrity, \CBROADCAST ensures all 3 properties. \\
  \textbf{Causal order:} From Theorem~\ref{theo:causal}, we must show
  that \CBROADCAST ensures both FIFO order and local order. \\
  \textbf{FIFO order:} Suppose a process A broadcasts $m$ before $m'$. Consider
  that a correct process B delivers $m'$. We must show that Process~B
  delivers $m$ before $m'$. \\
  Processes use added links only when they are safe. Thus, when Process~A
  broadcasts $m'$, direct neighbors already received or are guaranteed to
  receive $m$ beforehand. Since delivery order follows the receipt order, direct
  neighbors deliver $m$ before $m'$. They forward $m$ and $m'$ in this
  order. The rest of processes receive in this order. Process~B receives $m$
  before $m'$ either directly or transitively. Consequently, Process~B delivers
  $m$ before $m'$. \\
  \textbf{Local order:} Since broadcast and forward have identical instructions,
  the origin of broadcast messages does not matter. The proof is identical to
  that of FIFO order. \\
\end{proof}

\begin{theorem}[\CBROADCAST is a causal broadcast]
  \CBROADCAST is a causal broadcast in both static and dynamic network settings.
\end{theorem}

\begin{proof}
  For static networks, it comes from Theorem~\ref{theo:static}. For dynamic
  networks, it comes from Lemmas~\ref{lem:removals}~and~\ref{lem:additions}.
\end{proof}

Algorithm~\ref{algo:bufferbroadcast} ensures causal delivery of messages even in
dynamic network settings. Compared to the original causal broadcast for static
networks~\cite{friedman2004causal}, it uses an additional local structure:
buffers of messages. It associates a buffer to each new unsafe links. We assumed
that the size of these buffer stays small in general, for it depends of the time
taken by the ping phase which is assumed short. However, network conditions may
invalidate this assumption. Figure~\ref{fig:bufferproblem} depicts scenarios
where buffers grow out of acceptable boundaries. In
Figure~\ref{fig:bufferproblemA}, the issue comes from high transmission delays
from Process~A to Process~B, and from Process~B to Process~D compared to the
number of messages to broadcast and forward. The ping message $\pi$ did not
reach Process~D yet that the buffer contains a lot of messages. In
Figure~\ref{fig:bufferproblemB}, the issue comes from the departure of
Process~D. Depending on network settings, Process~A may not be able to detect
Process~D's departure. The former will never receive the awaited reply and the
buffer will grow forever. In Figure~\ref{fig:bufferproblemC}, the reply $\rho$
itself fails to reach Process~A. For the recall, this message can travel to
Process~A by any communication mean, including unreliable ones. If this fails,
Process~A's buffer to Process~D will grow
forever. %as long as Process~A receives or broadcasts
%messages.

\begin{algorithm}
  \input{input/algoboundingbuffer.tex}
  \caption{\label{algo:boundingbuffer}Bounding the size of buffers and handling
    network failures.}
\end{algorithm}

Algorithm~\ref{algo:boundingbuffer} solves the unbounded growth issue of
buffers. It solves the issue from the buffer owner's
perspective. Figure~\ref{fig:buffersolve} shows how this algorithm bounds the
size of buffers. In Figure~\ref{fig:buffersolveA}, Process~A broadcast $a$; then
wanted to add a link to Process~D so it sent a ping message; then broadcast $a'$
and $a''$ so it buffered them. We see that the ping message $\pi_1$ carries a
counter. The new buffer is identified by the same counter. In
Figure~\ref{fig:buffersolveB}, Process~A receives, delivers, and forwards the
message $x$. Each message delivery increases the size of current buffers. The
algorithm checks if the size of the buffer exceeds the configured bound (see
Line~\ref{line:maxsize}). Adding $x$ to the buffer would exceed the bound of
$2$ elements. It is first failure, so Process~A simply restarts the ping 
phase: it resets the buffer and sends another ping message $\pi_2$ (see
Line~\ref{line:reset}). The counter of the reset buffer is the one of the new
ping message. In the meantime, Process~D receives $\pi_1$ and sends the
corresponding reply $\rho_1$. In Figure~\ref{fig:buffersolveC}, Process~A
receives a broadcast message $y$. It delivers it, checks if the buffer can admit
it, adds the message to the buffer, and forwards it. Process~A also receives the
first reply $\rho_1$ but discards it, for no buffers have such counter. In
Figure~\ref{fig:buffersolveD}, Process~D receives $\pi_2$ and sends the
corresponding reply $\rho_2$ to Process~A. In Figure~\ref{fig:buffersolveE},
Process~A receives $\rho_2$. Since the corresponding buffer exists, it empties
it. The new link is now safe to use for causal broadcast.

While it solves the issue of unbounded buffers, it also brings another
issue. For instance, if the maximal size of buffers is too small, it could stuck
the protocol in a loop of retries. We address this issue by bounding the number
of retries. However, it means that the ping phase could fail
entirely. Causal broadcast must not employ the new link. In extreme cases, it
could cause partitions in the causal broadcast overlay network. It would violate
the uniform agreement property of causal broadcast. Thus, we assume a
sufficiently large maximal bound. It never creates partitions, for most links
become safe, and the failing ones are replaced over time thanks to
network dynamicity.

Other orthogonal improvements are possible. For instance, causal broadcast could
use reliable communication means to acknowledge the receipt of the ping
message. The time taken between the sending and the receipt of the
reply would increase when failures occur. However, it would take less
time than resetting the buffering phase.

\subsection{Complexity}
\label{subsec:complexity}

We review and discuss about the complexity of \CBROADCAST. We distinguish the
complexity brought by 
\begin{inparaenum}[(i)]
\item the overlay network,
\item reliable broadcast,
\item and causal ordering.
\end{inparaenum}

\noindent \textbf{Overlay network.} Processes cannot afford the upkeep of full
membership in large and dynamic systems. Instead, each process builds a partial
view the size of which is considerably smaller than the actual network size.  To
maintain these partial views, each process runs a peer-sampling
protocol~\cite{bertier-d2ht,jelasity2007gossip,jelasity2009tman}.  Some
peer-sampling protocols provides partial views the size of which scales
logarithmically with the actual network size~\cite{nedelec2017adaptive}.  The
number of messages forwarded by each process for each broadcast remains small,
for this number is equal to their view size: $O(Q)$ where $Q$ is the size of the
partial view.


\noindent \textbf{Reliable broadcast.} Gossiping constitutes an efficient mean
to disseminate messages to all
processes~\cite{demers1987epidemic},~\cite{birman1999bimodal}.
Algorithm~\ref{algo:reliablebroadcast} shows that it relies on a local structure
to guarantee that messages are delivered exactly once. This structure grows
linearly with the number of processes in the network: $O(N)$. In addition, each
message piggybacks a pair $\langle process,\, counter \rangle$ that identifies
it: $O(1)$. Checking if a message is a duplicate takes constant time: $O(1)$.

\noindent \textbf{Causal ordering.} Causal ordering primarily uses FIFO links to
broadcast messages which implies a constant size overhead on messages
$O(1)$. Its local space complexity is $O(W + B)$ where $W$ is the number of
messages awaiting on FIFO links, and $B$ is the size of the structure required
to ensure link safety. \CBROADCAST maintains one buffer per unsafe link during
its ping phase.  We assume that this time is short so the number of buffered
messages stays small. As shown in Section~\ref{sec:proposal}, network conditions
can make this assumption false. Algorithm~\ref{algo:boundingbuffer} allows to
bound the size of each buffer and handle network failures.

\noindent \textbf{Overall.}  Generated traffic remains the most important
criterion for scalability. The traffic generated by \CBROADCAST for each process
and for each broadcast only depends of the size of messages and the overlay
network chosen to broadcast messages. The size of messages is an irreducible
variable; and many protocols designed to build overlay networks achieve high
scalability in terms of network size and
dynamicity~\cite{bertier-d2ht,jelasity2007gossip,jelasity2009tman},~\cite{nedelec2017adaptive,voulgaris2005cyclon}.
Consequently, \CBROADCAST achieve high scalability in both these terms
too. \CBROADCAST is efficient, for the upper bound on the complexity of delivery
execution time does not depend of any factor.

However, to ensure causal order, \CBROADCAST may not use all outgoing links in
dynamic settings, for some may be temporarily unsafe. This negatively impacts
the overlay network properties. The next section shows an experiment that
highlights the influence of \CBROADCAST's way to ensure causal order on the
underlying overlay network. In particular, it shows that the number of hops
required by a broadcast message to reach all processes increases when delays on
transmission increase.




% \PAR{Causal ordering.}{Regarding message overhead, broadcast messages convey two 
%   types of control information: data to ensure FIFO order links, and
%   data to globally identify the message. Thus, 
%   message overhead is constant. 
%   % Regarding traffic, broadcast messages do not need to convey any control
%   % information. 
%   Regarding number of messages, processes must send each message to all their
%   neighborhood exactly once. It creates as many copies as neighbors. This number
%   of copies may constitute an issue when the size of messages is large. To solve
%   this issue, processes can send identifiers instead of large messages then send
%   these messages on demand.  It introduces additional delays in communications
%   but greatly reduces generated traffic~\cite{frey:hal-01479885}.  Regarding
%   local space consumption, our protocol maintains one buffer per link during its
%   ping phase time. We assume that this time is short so the number of
%   buffered messages stays small. As shown in Section~\ref{sec:proposal}, network
%   conditions can make this assumption false. Algorithm~\ref{algo:boundingbuffer}
%   allows to bound the size of each buffer and handle network failures.}


% % While Algorithm~\ref{algo:bufferbroadcast} shows no buffer management which
% % means that they can grow unbounded, we can easily bound them. For instance,
% % above a threshold we clear the buffer and send another locked message, or we
% % remove the link altogether.

% \PAR{Reliable broadcast.}{Algorithm~\ref{algo:reliablebroadcast} shows the
%   instructions of reliable broadcast. Even in presence of message duplicates it
%   avoids multiple deliveries of a same message. To achieve this, the most
%   straightforward structure is a set saving all new received messages. However,
%   it increases linearly with the number of delivered messages. Assigning a
%   unique identifier $\langle p,\, counter \rangle$ to each message changes the
%   complexity. It becomes a vector that increases linearly with the number of
%   processes that ever broadcast a message~\cite{fidge1988timestamps}. Using
%   interval tree clocks~\cite{almeida2008interval} slightly overloads messages
%   with identifiers but it improves the space complexity: the local structure
%   increases linearly with the number of processes that are currently involved in
%   broadcasting.}

% \begin{table}[h]
%   \begin{center}
%     \begin{tabularx}{0.5\columnwidth}{@{}cc@{}}
%       Message overhead & local space \\\hline
%       $O(1)$ & $O(N)$
%     \end{tabularx}
%   \end{center}
% \end{table}



% % \TODO{Rework.} Represented in Algorithms~\ref{algo:fifobroadcast}
% % and~\ref{algo:bufferbroadcast} by Function $alreadyReceived$. Causal ordering
% % and detecting duplicated receipts are orthogonal problems. In this paper, we do
% % not provide an implementation for the later. The simplest approach consists in
% % saving all received messages (\REF). However, the size of this set linearly and
% % monotonically increases as the number of broadcast messages increases. One would
% % prefer an approach based on vectors where one entry corresponds to the number of
% % messages received by a particular process~\cite{fidge1988timestamps}. Such
% % approach do not require to piggyback additional data in the message. However, it
% % requires to store locally a vector the size of which increases linearly compared
% % to the number of processes that ever broadcast a message. Interval tree
% % clocks~\cite{almeida2008interval} allow processes to reduce this complexity. It
% % becomes linear in terms of number of processes that are currently involved in
% % broadcasting. Possible improvements could take advantage of the fact that the
% % number of duplicates is equal to the number of incoming links. However, it does
% % not hold in dynamic networks where additional links are established. Finding a
% % sublinear bound for detecting duplicated receipts remains an open problem.


% \PAR{Overlay network.}{Values such as the number of messages sent by each
%   process, or the number of hops for a message to reach all processes, are
%   interdependent values brought by the overlay network. For instance, random
%   peer-sampling protocols~\cite{jelasity2007gossip} build network overlays with
%   properties similar to those of random graphs~\cite{erdos1959random}. They
%   provide each process with a random subset of neighbors the size of which is
%   logarithmically scaling with the network size. The number of messages sent by
%   each process for each broadcast message is logarithmic.  Since random
%   peer-sampling protocols build topologies close to random graphs, messages take
%   a logarithmic number of hops to reach all processes. In addition, random
%   peer-sampling protocols such as \SPRAY~\cite{nedelec2017adaptive} or
%   \CYCLON~\cite{voulgaris2005cyclon} create links using only
%   neighbor-to-neighbor interactions, i.e., they establish links only two hops
%   apart. It takes only 4 hops for new links to become safe.  The overhead
%   brought by these messages is negligible. The ping phase duration,
%   hence the size of buffers, remains small.}

% \begin{table}[h]
%   \begin{center}
%     \begin{tabularx}{0.5\columnwidth}{@{}Xcc@{}}
%       overlay & number of messages & meow\\\hline
%       $O(1)$ & $O(B)$ & \\
%     \end{tabularx}
%   \end{center}
% \end{table}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% End:
